{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bb24d2-f2f7-455f-8854-6d1ae1f233b8",
   "metadata": {},
   "source": [
    "# Custom evaluations of the fine tuned model locally\n",
    "\n",
    "After fine tuning and deploying your model for testing, you can compare it to the accuracy and faithfulness of responses from other models in RAG based systems.  If you'd like to compare your model to others and see how it performs, this notebook will help you do that.\n",
    "\n",
    "NOTE: If you'd like to do a standard evaluation using the llm-eval-app service, use the `eval_rh_api.ipynb` notebook.\n",
    "\n",
    "To prepare for the evaluation, you will need to have the following:\n",
    "1. A set of models deployed and accessible via an API.\n",
    "2. A config.yaml file with the model information.\n",
    "3. A set of reference questions and answers in a common format (csv, jsonl, or qna.yaml).\n",
    "4. A set of context data in PDF format.  These are generally the documents that the model was trained on.\n",
    "\n",
    "The process involves the following steps:\n",
    "1. Sanity check the models to ensure your configuration is working correctly.\n",
    "2. Generate reference questions and answers from the `reference_answers` directory.\n",
    "3. Generate sample context data using a Milvus Lite Vector DB and the PDFs in the `data_preparation/document_collection` directory.\n",
    "4. Get responses from each of the available models.\n",
    "5. Grade responses using InstructLab.\n",
    "6. Grade responses using OpenAI ChatGPT-4o as a Judge Model.\n",
    "7. Save the results and create a resulting score report in Excel, Markdown, and HTML.\n",
    "\n",
    "By the end of the notebook, you will have json file with the evaluation and a summary of the evaluation results in an Excel, Markdown, and HTML.\n",
    "\n",
    "#### Summary\n",
    "| question index   |   lab-tuned-granite |   lab-tuned-granite-rag |   granite-3.0-8b-instruct-rag |   gpt-4-rag |\n",
    "|:-----------------|--------------------:|------------------------:|------------------------------:|------------:|\n",
    "| Q1               |                   4 |                       5 |                             5 |     4       |\n",
    "| Q2               |                   1 |                       5 |                             5 |     5       |\n",
    "| ...              |                 ... |                     ... |                           ... |   ...       |\n",
    "| QX               |                   4 |                       5 |                             5 |     5       |\n",
    "| Sum              |                   9 |                      15 |                            15 |    14       |\n",
    "| Average          |                   3 |                       5 |                             5 |     4.66667 |\n",
    "\n",
    "\n",
    "#### lab-tuned-granite\n",
    "| user_input | reference | retrieved_context |  response |   score |     reasoning |\n",
    "|:-----------|----------:|------------------:|----------:|--------:|--------------:|\n",
    "| What is ...| It is...  | There is ...      | It is...  |  4      | The answer... |\n",
    "\n",
    "#### lab-tuned-granite-rag\n",
    "| user_input | reference | retrieved_context |  response |   score |     reasoning |\n",
    "|:-----------|----------:|------------------:|----------:|--------:|--------------:|\n",
    "| What is ...| It is...  | There is ...      | It is...  |  4      | The answer... |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308b229-b520-4e82-a783-eb921bb955e7",
   "metadata": {},
   "source": [
    "### Needed packages and imports\n",
    "\n",
    "The following packages are needed to run the evaluation service.  If you have not already installed them, you can do so by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e41b41-f60a-4b0f-91a1-cd273b60f21b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1140795af54d8",
   "metadata": {},
   "source": [
    "### Testing Configuration - `config.yaml`\n",
    "\n",
    "Before running the evaluation, you will need to create a `config.yaml` file with the model information.  There is [config_example.yaml](config_example.yaml) that has some <FIELDS> that need to be filled out, such as API Key, but you can use it to get started.\n",
    "\n",
    "The file should be in the following format:\n",
    "```yaml\n",
    "name: my-eval # this determines the output directory of the evaluation.\n",
    "judge:\n",
    "    model_name: gpt-4o # choose the best OpenAI model for judging the responses.\n",
    "    api_key: sk-12345  # OpenAI API Key is required to run the evaluations for both InstructLab and OpenAI\n",
    "    template: |        # This is the langchain scoring template for the judge model. It is used in the ChatGPT-4o model to score the responses. The InstructLab model uses its own scoring template.\n",
    "      Evaluate the answer_quality as:\n",
    "      - Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "      - Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "      ...\n",
    "testing_configs:\n",
    "  - name: lab-tuned-granite # this is a name for the testing configuration.  It also determines output file names.\n",
    "    endpoint_url: https://openai-api.com/v1 # The endpoint URL for the model. Ignore if using OpenAI. Don't forget /v1.\n",
    "    model_name: finetuned   # model name used by the OpenAI API. e.g. finetuned, gpt-4, etc.\n",
    "    model_type: vllm        # vllm/openai depending on the model type.  openai will ignore the endpoint_url\n",
    "    api_key: eyafsdasdfsdf  # API Key for the served model\n",
    "    rag: False              # Whether or not using RAG.  True if the template has context fields, False if it does not.\n",
    "    template: |\n",
    "      <|system|> I am a Red Hat Instruct Model\n",
    "      <|user|>\n",
    "      Answer the following question based on internal knowledge.\n",
    "      Question: {question}\n",
    "      Answer:\n",
    "      <|assistant|>\n",
    "  - name: gpt-4-rag\n",
    "    model_name: gpt-4\n",
    "    model_type: openai\n",
    "    api_key: SK-12345\n",
    "    rag: True\n",
    "    template: |\n",
    "      Context:\n",
    "      {context}\n",
    "      Answer the following question from the above context.\n",
    "      Question: {question}\n",
    "      Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fa004651738c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T18:52:20.509165Z",
     "start_time": "2025-02-05T18:52:20.501640Z"
    }
   },
   "outputs": [],
   "source": [
    "from eval_utils import get_config, check_judge_config, check_testing_config\n",
    "\n",
    "config = get_config()\n",
    "check_judge_config(config.get(\"judge\"))\n",
    "for testing_config in config.get(\"testing_configs\"):\n",
    "    check_testing_config(testing_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177bb49b96e57fe",
   "metadata": {},
   "source": [
    "## Sanity check models\n",
    "\n",
    "We will first test each of the models to ensure they are working correctly.  This will help us identify any issues with the configuration before running the evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4ccb5253478c0",
   "metadata": {},
   "source": [
    "#### Test Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c2ab24a267887b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T18:52:32.046902Z",
     "start_time": "2025-02-05T18:52:24.729482Z"
    }
   },
   "outputs": [],
   "source": [
    "from eval_utils import create_llm, chat_request, get_first_config\n",
    "\n",
    "for testing_config in config[\"testing_configs\"]:\n",
    "    print(\"-\" * 80)\n",
    "    print(testing_config.get(\"name\") or testing_config.get(\"model_name\"))\n",
    "    llm = create_llm(testing_config)\n",
    "    question = \"Who are you?\"\n",
    "    if testing_config.get(\"rag\"):\n",
    "        retrieved_context = \"Pretend to be a human named Bob\"\n",
    "    else:\n",
    "        retrieved_context = None\n",
    "    answer = chat_request(llm, testing_config.get(\"template\"), question, retrieved_context)\n",
    "    print(f\"Question: {question}? Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe9c8d763f89d3",
   "metadata": {},
   "source": [
    "## Generate Reference Data (Questions, Answers, and Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf034e7474f5bf5",
   "metadata": {},
   "source": [
    "### Use qna.yaml, csv, jsonl to create some data\n",
    "\n",
    "Before creating a set of reference ansers in a common `jsonl` format, you must:\n",
    "\n",
    "1. Put your reference answers in the `reference_answers` directory\n",
    "2. Put any relevant source PDF documents in the `data_preparation/document_collection`.\n",
    "\n",
    "The reference answers should be in the format of a csv, jsonl, or a qna.yaml file.  It's preferable to use questions and reference answers made by human subject matter experts.  To this end CSV and jsonl files are easy formats to work with.  A qna.yaml file can also be added as an easy option.\n",
    "\n",
    "The CSV should be formatted with `user_input` and `reference` fields.\n",
    "| user_input | reference |\n",
    "|:-----------|----------:|\n",
    "| What is ...| It is...  |\n",
    "\n",
    "The JSONL should be formatted with `user_input` and `reference` fields.\n",
    "```json lines\n",
    "{\"user_input\": \"What is ...\", \"reference\": \"It is...\"}\n",
    "{\"user_input\": \"What is ...\", \"reference\": \"It is...\"}\n",
    "```\n",
    "\n",
    "The YAML file should be formatted with `seed_examples` and `questions_and_answers` fields.  This mirrors the normal `qna.yaml` format so that you can reuse the qna.yaml from your taxonomy.\n",
    "```yaml\n",
    "seed_examples:\n",
    "    questions_and_answers:\n",
    "      - question: >\n",
    "          relevant question?\n",
    "        answer: >\n",
    "          reference answer\n",
    "      - question: >\n",
    "          relevant question 2?\n",
    "        answer: >\n",
    "          reference answer 2\n",
    "```\n",
    "After transforming the data, we will write the data to a `jsonl` file and add a `retrieved_context` field to the data. A Milvus Lite Vector DB will be generated from the PDFs in `data_preparation/document_collection`.  The context will be retrieved from the document collection.\n",
    "\n",
    "At this point you can inspect the `results/reference_answers.jsonl` file to see the data and fix any issues you see, such as manually fixing the `retrieved_context` field before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0570b77dfc0d23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T19:15:16.803993Z",
     "start_time": "2025-02-05T19:14:40.867561Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from eval_utils import get_output_dir, get_reference_answers, get_context, write_jsonl\n",
    "\n",
    "output_directory =  get_output_dir()\n",
    "reference_answers = get_reference_answers(\"./reference_answers\")\n",
    "reference_answers = get_context(reference_answers, \"../data_preparation/document_collection\")\n",
    "print(str(len(reference_answers)) + \" reference answers loaded\")\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "write_jsonl(f\"{output_directory}/reference_answers.jsonl\", reference_answers)\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cb58d65398ba",
   "metadata": {},
   "source": [
    "## Get responses from each of the available models\n",
    "\n",
    "Now that we have the `user_input`, `reference`, and `retrieved_context` fields in the `reference_answers.jsonl` file, we can get responses from each of the available models.  We will save the responses in a `responses` directory for each model.  The responses will be saved in a `jsonl` file with the format of `user_input`, `reference`, `retrieved_context`, and `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b82aa5d43363d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:58:28.394702Z",
     "start_time": "2025-02-05T20:58:28.383691Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from eval_utils import read_jsonl\n",
    "\n",
    "config = get_config()\n",
    "output_directory = get_output_dir()\n",
    "\n",
    "responses_directory = output_directory + \"/responses\"\n",
    "os.makedirs(responses_directory, exist_ok=True)\n",
    "\n",
    "reference_answers = read_jsonl(f\"{output_directory}/reference_answers.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318125c61600a8c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:59:23.160091Z",
     "start_time": "2025-02-05T20:58:30.766801Z"
    }
   },
   "outputs": [],
   "source": [
    "from eval_utils import chat_request, get_testing_config_name\n",
    "reference_answers_df = pd.DataFrame(reference_answers)\n",
    "\n",
    "for testing_config in config[\"testing_configs\"]:\n",
    "    print(\"-\" * 80)\n",
    "    print(testing_config.get(\"name\") or testing_config.get(\"model_name\"))\n",
    "    responses = reference_answers_df.copy()\n",
    "    responses[\"response\"] = \"\"\n",
    "    llm = create_llm(testing_config)\n",
    "    for index, row in responses.iterrows():\n",
    "        question = row[\"user_input\"]\n",
    "        print(f\"Question {index + 1}:\", question[:40])\n",
    "        if testing_config.get(\"rag\"):\n",
    "            retrieved_context = row[\"retrieved_context\"]\n",
    "        else:\n",
    "            retrieved_context = None\n",
    "        answer = chat_request(llm, testing_config.get(\"template\"), question, retrieved_context)\n",
    "        print(\"Answer: \" + answer[:40])\n",
    "        responses.at[index, \"response\"] = answer\n",
    "    testing_config_name = get_testing_config_name(testing_config)\n",
    "    responses.to_json(f\"{responses_directory}/{testing_config_name}_responses.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9817c4a2b04b2",
   "metadata": {},
   "source": [
    "## Grade responses using InstructLab\n",
    "\n",
    "Now that we have the responses from each of the models, we can grade the responses using InstructLab.  We will save the scores in a `ilab_scores` directory for each model.  The scores will be saved in a `jsonl` file with the format of `user_input`, `reference`, `retrieved_context`, `response`, `score`.  InstructLab will utilize Ragas along with ChatGPT-4o as a Judge Model to score the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27570f629f22ebbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:50:45.229935Z",
     "start_time": "2025-02-05T23:50:45.219387Z"
    }
   },
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "output_directory = get_output_dir()\n",
    "responses_directory = output_directory + \"/responses\"\n",
    "ilab_scores_directory = output_directory + \"/ilab_scores\"\n",
    "os.makedirs(ilab_scores_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343834679ec5001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:50:54.642503Z",
     "start_time": "2025-02-05T23:50:45.554628Z"
    }
   },
   "outputs": [],
   "source": [
    "from instructlab_ragas import ModelConfig, RagasEvaluator, RunConfig, Sample\n",
    "import os\n",
    "\n",
    "for testing_config in config[\"testing_configs\"]:\n",
    "    testing_config_name = get_testing_config_name(testing_config)\n",
    "    print(\"-\" * 80)\n",
    "    print(testing_config_name)\n",
    "\n",
    "    responses_filename = f\"{responses_directory}/{testing_config_name}_responses.jsonl\"\n",
    "    print(responses_filename)\n",
    "    responses = pd.read_json(responses_filename, orient=\"records\", lines=True)\n",
    "    responses_list = responses[[\"user_input\", \"reference\", \"response\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = config[\"judge\"][\"api_key\"]\n",
    "    evaluator = RagasEvaluator()\n",
    "    evaluation_result = evaluator.run(dataset=responses_list)\n",
    "\n",
    "    scores = pd.DataFrame(responses_list)\n",
    "    scores[\"score\"] = [score[\"domain_specific_rubrics\"] for score in evaluation_result.scores]\n",
    "    scores_filename = f\"{ilab_scores_directory}/{testing_config_name}_scores\"\n",
    "    scores.to_json(f\"{scores_filename}.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74f2910d1b7718",
   "metadata": {},
   "source": [
    "## Grade responses using OpenAI ChatGPT-4o as a Judge Model\n",
    "\n",
    "Alternatively, you can customize and score the responses using OpenAI ChatGPT-4o as a judge model and your own custom template from the `judge` field in the config.yaml.\n",
    "\n",
    "```yaml\n",
    "name: my-eval # this determines the output directory of the evaluation.\n",
    "judge:\n",
    "  endpoint_url: '' # defaults to OpenAI API endpoint\n",
    "  model_name: gpt-4o\n",
    "  api_key: your-openai-key\n",
    "  template: |\n",
    "    You are an evaluation system tasked with assessing the answer quality of a AI generated response in relation to the posed question and reference answer. Assess if the response is correct, accurate, and factual based on the reference answer.\n",
    "    For evaluating factuality of the answer look at the reference answer compare the model answer to it.\n",
    "    Evaluate the answer_quality as:\n",
    "    - Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "    - Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "    - Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "    - Score 4: The response is mostly correct, accurate, and factual.\n",
    "    - Score 5: The response is completely correct, accurate, and factual.\n",
    "    Here is the question: \\n ------- \\n {question} \\n -------\n",
    "    Here is model answer: \\n ------- \\n {answer} \\n -------\n",
    "    Here is the reference answer(may be very short and lack details or indirect, long and extractive):  \\n ------- \\n {reference_answer} \\n ------- \\n\n",
    "    Assess the quality of model answer with respect to the Reference Answer, but do not penalize the model answer for adding details or give a direct answer to user question.\n",
    "    Approach your evaluation in step-by-step manner.\n",
    "    For evaluating first list out keys facts covered in the reference answer and check how many are covered by the model answer.\n",
    "    If the question or reference answer is about steps then check if the steps and their order in model answer match with reference answer.\n",
    "    Provide your response as JSON object with two keys: 'reasoning' and 'answer_quality'.\n",
    "```\n",
    "\n",
    "From this template, ChatGPT-4o will return a JSON object with the `answer_quality` and `reasoning` fields.  The `answer_quality` field will be a score between 1 and 5, with 5 being the best score.  The `reasoning` field will provide a reason for the score.\n",
    "\n",
    "We will save the scores in a `openai_scores` directory for each model.  The scores will be saved in a `jsonl` file with the format of `user_input`, `reference`, `retrieved_context`, `response`, `score`, and `reasoning`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd38e7861cf4ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:53:20.385085Z",
     "start_time": "2025-02-05T23:53:20.375865Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from eval_utils import get_config, get_output_dir\n",
    "\n",
    "config = get_config()\n",
    "output_directory = get_output_dir()\n",
    "responses_directory = output_directory + \"/responses\"\n",
    "openai_scores_directory = output_directory + \"/openai_scores\"\n",
    "os.makedirs(openai_scores_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e356f321ec521d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:53:20.827569Z",
     "start_time": "2025-02-05T23:53:20.825830Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "scoring_template_str = config[\"judge\"].get(\"template\")\n",
    "assert scoring_template_str\n",
    "SCORING_PROMPT = PromptTemplate.from_template(scoring_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7a15b14cd0bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:53:21.432796Z",
     "start_time": "2025-02-05T23:53:21.421204Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "judge_client = OpenAI(api_key=config[\"judge\"][\"api_key\"])\n",
    "judge_model_name = config[\"judge\"][\"model_name\"]\n",
    "\n",
    "def openai_score_request(question, answer, reference_answer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": SCORING_PROMPT.format(\n",
    "                question=question,\n",
    "                answer=answer,\n",
    "                reference_answer=reference_answer\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    completion = judge_client.chat.completions.create(\n",
    "        model=judge_model_name,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    response_content = completion.choices[0].message.content\n",
    "    response_content = re.sub(r'^```json', '', response_content)\n",
    "    response_content = re.sub(r'```$', '', response_content)\n",
    "    try:\n",
    "        result = json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        result = {\"answer_quality\": 0, \"reasoning\": \"Error\"}\n",
    "        print(\"response_content:\", response_content)\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    score = result[\"answer_quality\"]\n",
    "    reasoning = result[\"reasoning\"]\n",
    "    return score, reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6786d88e859ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T00:00:16.695709Z",
     "start_time": "2025-02-05T23:59:37.251149Z"
    }
   },
   "outputs": [],
   "source": [
    "from eval_utils import replace_special_char\n",
    "\n",
    "for testing_config in config[\"testing_configs\"]:\n",
    "    testing_config_name = get_testing_config_name(testing_config)\n",
    "    print(\"-\" * 80)\n",
    "    print(testing_config_name)\n",
    "\n",
    "    responses_filename = f\"{responses_directory}/{testing_config_name}_responses.jsonl\"\n",
    "    scores = pd.read_json(responses_filename, orient=\"records\", lines=True)\n",
    "    scores[\"score\"] = None\n",
    "    scores[\"reasoning\"] = None\n",
    "\n",
    "    for index, row in scores.iterrows():\n",
    "        user_input = row[\"user_input\"]\n",
    "        response = row[\"response\"]\n",
    "        reference_answer = row[\"reference\"]\n",
    "        print(f\"Question {index + 1}:\", user_input)\n",
    "        if response:\n",
    "            score, reasoning = openai_score_request(user_input, response, reference_answer)\n",
    "            scores.at[index, \"score\"] = score\n",
    "            scores.at[index, \"reasoning\"] = reasoning\n",
    "            print(\"Answer:\", response[:80])\n",
    "            print(\"Score:\", score, reasoning[:80])\n",
    "\n",
    "    judge_name = replace_special_char(judge_model_name)\n",
    "    scores_filename = f\"{openai_scores_directory}/{testing_config_name}_scores\"\n",
    "    scores.to_json(f\"{scores_filename}.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026ec6e6c8ba3e5",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Now that we have the scores for each of the models, we can save the results in the `evaluation.json` file. The results will include the `reference_answers`, `ilab_evaluation`, and `openai_evaluation` fields.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"reference_answers\": [\n",
    "        {\"user_input\": \"What is ...\", \"reference\": \"It is...\", \"retrieved_context\": \"There is ...\"},\n",
    "        {\"user_input\": \"What is ...\",\"reference\": \"It is...\",\"retrieved_context\": \"There is ...\"}\n",
    "    ],\n",
    "    \"ilab_evaluation\": {\n",
    "        \"status\": \"complete\",\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"name\": \"lab-tuned-granite\",\n",
    "                \"scores\": [\n",
    "                    {\"user_input\": \"What is ...\", ..., \"score\": 4},\n",
    "                    {\"user_input\": \"What is ...\", ..., \"score\": 4},\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"openai_evaluation\": {\n",
    "        \"status\": \"complete\",\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"name\": \"other-model-rag\",\n",
    "                \"scores\": [\n",
    "                    {\"user_input\": \"What is ...\", ..., \"score\": 4,\"reasoning\": \"The answer...\"}\n",
    "                    {\"user_input\": \"What is ...\", ..., \"score\": 4,\"reasoning\": \"The answer...\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3934047999521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T00:21:50.733182Z",
     "start_time": "2025-02-06T00:21:50.708079Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = get_config()\n",
    "output_directory = get_output_dir()\n",
    "responses_directory = output_directory + \"/responses\"\n",
    "ilab_scores_directory = output_directory + \"/ilab_scores\"\n",
    "openai_scores_directory = output_directory + \"/openai_scores\"\n",
    "os.makedirs(openai_scores_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_eval_results(directory):\n",
    "    results = []\n",
    "    for testing_config in config[\"testing_configs\"]:\n",
    "        testing_config_name = get_testing_config_name(testing_config)\n",
    "        scores = pd.read_json(f\"{directory}/{testing_config_name}_scores.jsonl\", orient=\"records\", lines=True)\n",
    "        results.append({\n",
    "            \"name\": testing_config.get(\"name\") or testing_config.get(\"model_name\"),\n",
    "            \"scores\": scores.to_dict(orient=\"records\")\n",
    "        })\n",
    "    return {\n",
    "        \"status\": \"complete\",\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "evaluation = {}\n",
    "evaluation[\"reference_answers\"] = read_jsonl(f\"{output_directory}/reference_answers.jsonl\")\n",
    "evaluation[\"ilab_evaluation\"] = read_eval_results(ilab_scores_directory)\n",
    "evaluation[\"openai_evaluation\"] = read_eval_results(openai_scores_directory)\n",
    "json.dump(evaluation, open(f\"{output_directory}/evaluation.json\", 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c395a7e84df039f",
   "metadata": {},
   "source": [
    "## Create resulting score report Excel / Markdown / HTML\n",
    "\n",
    "Now that the evaluation is complete, we can summarize the results in an Excel, Markdown, and HTML file for both the InstructLab evaluation and the OpenAI evaluation.  Feel free to use either.  You can find the files in the `results` directory and inspect the results.  The summary scores are between 1 and 5, with 5 being the best score.  The first table is a summary for each model and each model detail, including all the data follows.  If you're worried about the results, this should help diagnose any issues like subpar context retrieval.\n",
    "\n",
    "#### Summary\n",
    "| question index   |   lab-tuned-granite |   lab-tuned-granite-rag |   granite-3.0-8b-instruct-rag | gpt-4-rag |\n",
    "|:-----------------|--------------------:|------------------------:|------------------------------:|----------:|\n",
    "| Q1               |                   4 |                       5 |                             5 |         4 |\n",
    "| Q2               |                   1 |                       5 |                             5 |         5 |\n",
    "| ...              |                 ... |                     ... |                           ... |       ... |\n",
    "| QX               |                   4 |                       5 |                             5 |         5 |\n",
    "| Sum              |                   9 |                      15 |                            15 |        14 |\n",
    "| Average          |                   3 |                       5 |                             5 |   4.66667 |\n",
    "\n",
    "\n",
    "#### lab-tuned-granite\n",
    "| user_input | reference | retrieved_context |  response |   score |     reasoning |\n",
    "|:-----------|----------:|------------------:|----------:|--------:|--------------:|\n",
    "| What is ...| It is...  | There is ...      | It is...  |  4      | The answer... |\n",
    "\n",
    "#### lab-tuned-granite-rag\n",
    "| user_input | reference | retrieved_context |  response |   score |     reasoning |\n",
    "|:-----------|----------:|------------------:|----------:|--------:|--------------:|\n",
    "| What is ...| It is...  | There is ...      | It is...  |  4      | The answer... |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15ce5ad8037cde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T00:28:53.503718Z",
     "start_time": "2025-02-06T00:28:53.496573Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_directory = get_output_dir()\n",
    "eval = json.load(open(f\"{output_directory}/evaluation.json\"))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from eval_utils import summarize_results, write_excel, write_markdown, write_html\n",
    "\n",
    "ilab_summary_output_df = summarize_results(eval.get(\"ilab_evaluation\").get(\"results\"))\n",
    "openai_summary_output_df = summarize_results(eval.get(\"openai_evaluation\").get(\"results\"))\n",
    "\n",
    "write_excel(\n",
    "    ilab_summary_output_df,\n",
    "    eval.get(\"ilab_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/ilab_scores.xlsx\"\n",
    ")\n",
    "\n",
    "write_excel(\n",
    "    openai_summary_output_df,\n",
    "    eval.get(\"openai_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/openai_scores.xlsx\"\n",
    ")\n",
    "\n",
    "write_markdown(\n",
    "    ilab_summary_output_df,\n",
    "    eval.get(\"ilab_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/ilab_scores.md\"\n",
    ")\n",
    "\n",
    "write_markdown(\n",
    "    openai_summary_output_df,\n",
    "    eval.get(\"openai_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/openai_scores.md\"\n",
    ")\n",
    "\n",
    "write_html(\n",
    "    ilab_summary_output_df,\n",
    "    eval.get(\"ilab_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/ilab_scores.html\"\n",
    ")\n",
    "\n",
    "write_html(\n",
    "    openai_summary_output_df,\n",
    "    eval.get(\"openai_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/openai_scores.html\"\n",
    ")\n"
   ],
   "id": "a6cee5ca896457a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
