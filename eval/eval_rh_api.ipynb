{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bb24d2-f2f7-455f-8854-6d1ae1f233b8",
   "metadata": {},
   "source": [
    "# Evaluating the fine tuned model using the evaluation service\n",
    "\n",
    "After fine tuning and deploying your model for testing, you can compare it to the accuracy and faithfulness of responses from other models in RAG based systems.  This notebook will guide you through the steps of evaluating your model using the evaluation service.\n",
    "\n",
    "NOTE: If you'd like to do a custom evaluation using your own comparison models and configurations, use the `local_custom_eval.ipynb` notebook.\n",
    "\n",
    "To prepare for the evaluation, you will need to have the following:\n",
    "1. A fine-tuned model deployed and accessible via an API.\n",
    "2. A set of reference questions and answers in a common format (csv, jsonl, or qna.yaml).\n",
    "3. A set of context data in PDF format.  These are generally the documents that the model was trained on.\n",
    "\n",
    "The process involves the following steps:\n",
    "1. Load the InstructLab tuned model using and test it with a simple question to ensure it is working.\n",
    "2. Generate reference questions and answers from the `reference_answers` directory.\n",
    "3. Generate sample context data using a Milvus Lite Vector DB and the PDFs in the `data_preparation/document_collection` directory.\n",
    "4. Create an evaluation using the reference answers and the InstructLab tuned model.\n",
    "5. Wait for the evaluation to complete.\n",
    "6. Summarize the results in an Excel, Markdown, and HTML.\n",
    "\n",
    "By the end of the notebook, you will have json file with the evaluation and a summary of the evaluation results in an Excel, Markdown, and HTML.  By default be stored in the `results` directory.\n",
    "```\n",
    ".\n",
    "├── data_preparation\n",
    "│   ├── document_collection\n",
    "│   │   └── sample.pdf\n",
    "│   └── taxonomy\n",
    "│       └── knowledge\n",
    "└── eval\n",
    "    ├── eval_rh_api.ipynb\n",
    "    ├── reference_answers\n",
    "    │   ├── sample.csv\n",
    "    │   ├── sample.jsonl\n",
    "    │   └── sample.yaml\n",
    "    └── results\n",
    "        ├── evaluation.json\n",
    "        ├── ilab_scores.xlsx\n",
    "        ├── openai_scores.xlsx\n",
    "        └── reference_answers.jsonl\n",
    "```\n",
    "\n",
    "The `evaluation.json` will contain an evaluation run for ilab and for a plain OpenAI generated score.\n",
    "```\n",
    "{\n",
    "    \"id\": \"20250203175239362956\",\n",
    "    \"reference_answers\": [...],\n",
    "    \"openai_evaluation\": {\n",
    "        \"status\": \"complete\",\n",
    "        \"results\": [...]\n",
    "    },\n",
    "    \"ilab_evaluation\": {\n",
    "        \"status\": \"complete\",\n",
    "        \"results\": [...]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "And the output files will contain the summary of the evaluation results.\n",
    "\n",
    "#### Summary\n",
    "| question index   |   lab-tuned-granite |   lab-tuned-granite-rag |   granite-3.0-8b-instruct-rag |   gpt-4-rag |\n",
    "|:-----------------|--------------------:|------------------------:|------------------------------:|------------:|\n",
    "| Q1               |                   4 |                       5 |                             5 |     4       |\n",
    "| Q2               |                   1 |                       5 |                             5 |     5       |\n",
    "| ...              |                 ... |                     ... |                           ... |   ...       |\n",
    "| QX               |                   4 |                       5 |                             5 |     5       |\n",
    "| Sum              |                   9 |                      15 |                            15 |    14       |\n",
    "| Average          |                   3 |                       5 |                             5 |     4.66667 |\n",
    "\n",
    "\n",
    "#### lab-tuned-granite\n",
    "| user_input | reference | retrieved_context |  response |   score |     reasoning |\n",
    "|:-----------|----------:|------------------:|----------:|--------:|--------------:|\n",
    "| What is ...| It is...  | There is ...      | It is...  |  4      | The answer... |\n",
    "\n",
    "#### lab-tuned-granite-rag\n",
    "| user_input | reference | retrieved_context |  response |   score |     reasoning |\n",
    "|:-----------|----------:|------------------:|----------:|--------:|--------------:|\n",
    "| What is ...| It is...  | There is ...      | It is...  |  4      | The answer... |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308b229-b520-4e82-a783-eb921bb955e7",
   "metadata": {},
   "source": [
    "### Needed packages and imports\n",
    "\n",
    "The following packages are needed to run the evaluation service.  If you have not already installed them, you can do so by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e41b41-f60a-4b0f-91a1-cd273b60f21b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1140795af54d8",
   "metadata": {},
   "source": [
    "### Testing Configuration - Environment Variables\n",
    "\n",
    "The following environment variables are needed to run the evaluation service.  If you have not already set them, you can do so by creating a `.env` file in the root of the project and adding the following variables:\n",
    "\n",
    "```\n",
    "FINETUNED_MODEL_URL=https://finetuned-model-api.server.com/v1\n",
    "FINETUNED_MODEL_NAME=finetuned_model_name\n",
    "FINETUNED_MODEL_API_KEY=model-api-key\n",
    "EVAL_SERVICE_URL=https://eval-service-api.server.com\n",
    "EVAL_SERVICE_API_KEY=eval-service-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fa004651738c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "output_directory = os.getenv(\"OUTPUT_DIR\", \"results\")\n",
    "\n",
    "eval_service_url = os.getenv(\"EVAL_SERVICE_URL\")\n",
    "eval_service_api_key = os.getenv(\"EVAL_SERVICE_API_KEY\")\n",
    "\n",
    "ilab_tuned_model_url = os.getenv(\"ILAB_TUNED_MODEL_URL\")\n",
    "ilab_tuned_model_name = os.getenv(\"ILAB_TUNED_MODEL_NAME\")\n",
    "ilab_tuned_model_api_key = os.getenv(\"ILAB_TUNED_MODEL_API_KEY\")\n",
    "\n",
    "if not all([\n",
    "    output_directory,\n",
    "    eval_service_url,\n",
    "    eval_service_api_key,\n",
    "    ilab_tuned_model_url,\n",
    "    ilab_tuned_model_name,\n",
    "    ilab_tuned_model_api_key\n",
    "]):\n",
    "    raise ValueError(\"One or more required variables are empty.  \"\n",
    "                     \"Please check your environment settings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177bb49b96e57fe",
   "metadata": {},
   "source": [
    "## Sanity check model\n",
    "\n",
    "We will first test the InstructLab tuned model to ensure it is working correctly.  We will use a simple question to test the model.  If you're curious about the code, you can find it in the `eval_utils.py` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4ccb5253478c0",
   "metadata": {},
   "source": [
    "#### Test Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3672da13d05a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import create_llm, chat_request, ILAB_TUNED_MODEL_PROMPT, ILAB_TUNED_MODEL_RAG_PROMPT\n",
    "\n",
    "llm = create_llm({\n",
    "    \"endpoint_url\": ilab_tuned_model_url,\n",
    "    \"model_name\": ilab_tuned_model_name,\n",
    "    \"api_key\": ilab_tuned_model_api_key\n",
    "})\n",
    "\n",
    "question = \"Who are you?\"\n",
    "answer = chat_request(llm, ILAB_TUNED_MODEL_PROMPT, question)\n",
    "print(answer, \"\\n\")\n",
    "\n",
    "answer = chat_request(llm, ILAB_TUNED_MODEL_RAG_PROMPT, question, context=\"Pretend to be a human named Bob\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe9c8d763f89d3",
   "metadata": {},
   "source": [
    "## Generate Reference Data (Questions, Answers, and Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf034e7474f5bf5",
   "metadata": {},
   "source": [
    "### Use qna.yaml, csv, jsonl to create some data\n",
    "\n",
    "Before creating a set of reference ansers in a common `jsonl` format, you must:\n",
    "\n",
    "1. Put your reference answers in the `reference_answers` directory\n",
    "2. Put any relevant source PDF documents in the `data_preparation/document_collection`.\n",
    "\n",
    "The reference answers should be in the format of a csv, jsonl, or a qna.yaml file.  It's preferable to use questions and reference answers made by human subject matter experts.  To this end CSV and jsonl files are easy formats to work with.  A qna.yaml file can also be added as an easy option.\n",
    "\n",
    "The CSV should be formatted with `user_input` and `reference` fields.\n",
    "| user_input | reference |\n",
    "|:-----------|----------:|\n",
    "| What is ...| It is...  |\n",
    "\n",
    "The JSONL should be formatted with `user_input` and `reference` fields.\n",
    "```json lines\n",
    "{\"user_input\": \"What is ...\", \"reference\": \"It is...\"}\n",
    "{\"user_input\": \"What is ...\", \"reference\": \"It is...\"}\n",
    "```\n",
    "\n",
    "The YAML file should be formatted with `seed_examples` and `questions_and_answers` fields.  This mirrors the normal `qna.yaml` format so that you can reuse the qna.yaml from your taxonomy.\n",
    "```yaml\n",
    "seed_examples:\n",
    "    questions_and_answers:\n",
    "      - question: >\n",
    "          relevant question?\n",
    "        answer: >\n",
    "          reference answer\n",
    "      - question: >\n",
    "          relevant question 2?\n",
    "        answer: >\n",
    "          reference answer 2\n",
    "```\n",
    "After transforming the data, we will write the data to a `jsonl` file and add a `retrieved_context` field to the data. A Milvus Lite Vector DB will be generated from the PDFs in `data_preparation/document_collection`.  The context will be retrieved from the document collection.\n",
    "\n",
    "At this point you can inspect the `results/reference_answers.jsonl` file to see the data and fix any issues you see, such as manually fixing the `retrieved_context` field before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0570b77dfc0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import get_reference_answers, get_context, write_jsonl, read_jsonl\n",
    "\n",
    "reference_answers = get_reference_answers(\"./reference_answers\")\n",
    "reference_answers = get_context(reference_answers, \"../data_preparation/document_collection\")\n",
    "print(str(len(reference_answers)) + \" reference answers loaded\")\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "write_jsonl(f\"{output_directory}/reference_answers.jsonl\", reference_answers)\n",
    "\n",
    "print(\"user_input:\", reference_answers[-1][\"user_input\"])\n",
    "print(\"reference:\", reference_answers[-1][\"reference\"])\n",
    "print(\"retrieved_context:\", reference_answers[-1][\"retrieved_context\"][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900fe594a136afb",
   "metadata": {},
   "source": [
    "## Prepare API Request\n",
    "\n",
    "Now that we have the reference answers, we can create an evaluation request to the evaluation service.  The request will need the InstructLab tuned model information, the reference answers, and the evaluation service API url and key which we set as environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc669beef68f158",
   "metadata": {},
   "source": [
    "### Create Evaluation\n",
    "\n",
    " We'll create the evaluation, but it will take some time to complete.  We'll check the status of the evaluation in the next step using the evaluation id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de138716fe6eff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from eval_utils import read_jsonl\n",
    "\n",
    "\n",
    "def create_evaluation(reference_answers: list[dict]) -> str:\n",
    "    eval_service_url = os.getenv(\"EVAL_SERVICE_URL\")\n",
    "    eval_service_api_key = os.getenv(\"EVAL_SERVICE_API_KEY\")\n",
    "\n",
    "    ilab_tuned_model_url = os.getenv(\"ILAB_TUNED_MODEL_URL\")\n",
    "    ilab_tuned_model_name = os.getenv(\"ILAB_TUNED_MODEL_NAME\")\n",
    "    ilab_tuned_model_api_key = os.getenv(\"ILAB_TUNED_MODEL_API_KEY\")\n",
    "\n",
    "\n",
    "    request_data = {\n",
    "        \"endpoint_url\": ilab_tuned_model_url,\n",
    "        \"model_name\": ilab_tuned_model_name,\n",
    "        \"api_key\": ilab_tuned_model_api_key,\n",
    "        \"reference_answers\": reference_answers\n",
    "    }\n",
    "\n",
    "    post_url = urljoin(eval_service_url, \"/api/evaluations\")\n",
    "\n",
    "    response = requests.post(\n",
    "        post_url,\n",
    "        json=request_data,\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"api-key\": eval_service_api_key\n",
    "        })\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "reference_answers = read_jsonl(f\"{output_directory}/reference_answers.jsonl\")\n",
    "eval = create_evaluation(reference_answers)\n",
    "eval_id = eval.get(\"id\")\n",
    "eval_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebd6ddb4f76f0e",
   "metadata": {},
   "source": [
    "### Wait for Evaluation\n",
    "\n",
    "There are two evaluations that will be run, one for the InstructLab native evaluation and one for a plain OpenAI generated score.  We will wait for both evaluations to complete before moving on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aded186c53ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "def wait_for_evaluation(eval_id, eval_type):\n",
    "    eval_service_url = os.getenv(\"EVAL_SERVICE_URL\")\n",
    "    eval_service_api_key = os.getenv(\"EVAL_SERVICE_API_KEY\")\n",
    "\n",
    "    get_url = urljoin(eval_service_url, f\"/api/evaluations/{eval_id}\")\n",
    "    seconds_between_requests = 15\n",
    "\n",
    "    # every 15 seconds get the evaluation status\n",
    "    eval_status = \"new\"\n",
    "    eval = None\n",
    "    while eval_status != \"complete\":\n",
    "        response = requests.get(\n",
    "            get_url,\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"api-key\": eval_service_api_key\n",
    "            })\n",
    "\n",
    "        eval = response.json()\n",
    "        eval_status = eval.get(eval_type).get(\"status\")\n",
    "        print(f'Eval {eval_type} for {eval_id} is \"{eval_status}\".')\n",
    "        if eval_status != \"complete\":\n",
    "            print('Waiting for evaluation to complete...')\n",
    "            time.sleep(seconds_between_requests)\n",
    "    return eval\n",
    "\n",
    "eval = wait_for_evaluation(eval_id, \"ilab_evaluation\")\n",
    "eval = wait_for_evaluation(eval_id, \"openai_evaluation\")\n",
    "\n",
    "with open(f\"{output_directory}/evaluation.json\", 'w') as json_file:\n",
    "    json.dump(eval, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c395a7e84df039f",
   "metadata": {},
   "source": [
    "## Create resulting score report Excel / Markdown / HTML\n",
    "\n",
    "Now that the evaluation is complete, we can summarize the results in an Excel, Markdown, and HTML file for both the InstructLab evaluation and the OpenAI evaluation.  Feel free to use either.  You can find the files in the `results` directory and inspect the results.  The summary scores are between 1 and 5, with 5 being the best score.  The first table is a summary for each model and each model detail, including all the data follows.  If you're worried about the results, this should help diagnose any issues like subpar context retrieval.\n",
    "\n",
    "#### Summary\n",
    "| question index   |   lab-tuned-granite |   lab-tuned-granite-rag |   granite-3.0-8b-instruct-rag | gpt-4-rag |\n",
    "|:-----------------|--------------------:|------------------------:|------------------------------:|----------:|\n",
    "| Q1               |                   4 |                       5 |                             5 |         4 |\n",
    "| Q2               |                   1 |                       5 |                             5 |         5 |\n",
    "| ...              |                 ... |                     ... |                           ... |       ... |\n",
    "| QX               |                   4 |                       5 |                             5 |         5 |\n",
    "| Sum              |                   9 |                      15 |                            15 |        14 |\n",
    "| Average          |                   3 |                       5 |                             5 |   4.66667 |\n",
    "\n",
    "\n",
    "#### lab-tuned-granite\n",
    "| user_input | reference | retrieved_context |  response |   score |     reasoning |\n",
    "|:-----------|----------:|------------------:|----------:|--------:|--------------:|\n",
    "| What is ...| It is...  | There is ...      | It is...  |  4      | The answer... |\n",
    "\n",
    "#### lab-tuned-granite-rag\n",
    "| user_input | reference | retrieved_context |  response |   score |     reasoning |\n",
    "|:-----------|----------:|------------------:|----------:|--------:|--------------:|\n",
    "| What is ...| It is...  | There is ...      | It is...  |  4      | The answer... |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15ce5ad8037cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "eval = json.load(open(f\"{output_directory}/evaluation.json\"))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from eval_utils import summarize_results, write_excel, write_markdown, write_html\n",
    "\n",
    "ilab_summary_output_df = summarize_results(eval.get(\"ilab_evaluation\").get(\"results\"))\n",
    "openai_summary_output_df = summarize_results(eval.get(\"openai_evaluation\").get(\"results\"))\n",
    "\n",
    "write_excel(\n",
    "    ilab_summary_output_df,\n",
    "    eval.get(\"ilab_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/ilab_scores.xlsx\"\n",
    ")\n",
    "\n",
    "write_excel(\n",
    "    openai_summary_output_df,\n",
    "    eval.get(\"openai_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/openai_scores.xlsx\"\n",
    ")\n",
    "\n",
    "write_markdown(\n",
    "    ilab_summary_output_df,\n",
    "    eval.get(\"ilab_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/ilab_scores.md\"\n",
    ")\n",
    "\n",
    "write_markdown(\n",
    "    openai_summary_output_df,\n",
    "    eval.get(\"openai_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/openai_scores.md\"\n",
    ")\n",
    "\n",
    "write_html(\n",
    "    ilab_summary_output_df,\n",
    "    eval.get(\"ilab_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/ilab_scores.html\"\n",
    ")\n",
    "\n",
    "write_html(\n",
    "    openai_summary_output_df,\n",
    "    eval.get(\"openai_evaluation\").get(\"results\"),\n",
    "    f\"{output_directory}/openai_scores.html\"\n",
    ")\n"
   ],
   "id": "511a2ebbcd1cb162"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
